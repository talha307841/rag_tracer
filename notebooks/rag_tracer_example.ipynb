{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Tracer Example: OpenAI + pgvector Integration\n",
    "\n",
    "This notebook demonstrates how to integrate the RAG Tracer SDK with an OpenAI-based RAG pipeline using pgvector for document storage and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install openai pgvector psycopg2-binary scikit-learn rag-tracer-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import openai\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tracer_sdk.tracer import RAGTracer, EmbeddingData, RetrievalData, ResponseData, TelemetryData\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up OpenAI client\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up RAG Tracer\n",
    "tracer = RAGTracer(api_url=\"http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample documents for our RAG system\n",
    "documents = [\n",
    "    {\"id\": \"doc1\", \"content\": \"Elon Musk is the CEO of Tesla and SpaceX.\"},\n",
    "    {\"id\": \"doc2\", \"content\": \"Tesla is an electric vehicle manufacturer founded by Elon Musk.\"},\n",
    "    {\"id\": \"doc3\", \"content\": \"SpaceX is a space exploration company founded by Elon Musk.\"},\n",
    "    {\"id\": \"doc4\", \"content\": \"Apple Inc. is a technology company led by CEO Tim Cook.\"},\n",
    "    {\"id\": \"doc5\", \"content\": \"Microsoft is a technology company led by CEO Satya Nadella.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to generate embeddings using OpenAI\n",
    "def generate_embedding(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to retrieve documents using pgvector\n",
    "def retrieve_documents(query_embedding, top_k=3):\n",
    "    # In a real implementation, this would connect to a PostgreSQL database with pgvector\n",
    "    # For this example, we'll simulate retrieval\n",
    "    \n",
    "    # Generate embeddings for all documents\n",
    "    doc_embeddings = [generate_embedding(doc[\"content\"]) for doc in documents]\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    # Get top-k documents\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    retrieval_results = []\n",
    "    for i in top_indices:\n",
    "        retrieval_results.append({\n",
    "            \"document_id\": documents[i][\"id\"],\n",
    "            \"similarity_score\": float(similarities[i]),\n",
    "            \"content\": documents[i][\"content\"]\n",
    "        })\n",
    "    \n",
    "    return retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to generate response using OpenAI\n",
    "def generate_response(query, retrieved_docs):\n",
    "    context = \"\\n\".join([doc[\"content\"] for doc in retrieved_docs])\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text.strip(), prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Main RAG pipeline with tracing\n",
    "def rag_pipeline(query):\n",
    "    # Record start time for telemetry\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Generate embedding for query\n",
    "    embedding_start = time.time()\n",
    "    query_embedding = generate_embedding(query)\n",
    "    embedding_latency = (time.time() - embedding_start) * 1000\n",
    "    \n",
    "    # 2. Retrieve documents\n",
    "    retrieval_start = time.time()\n",
    "    retrieved_docs = retrieve_documents(query_embedding)\n",
    "    retrieval_latency = (time.time() - retrieval_start) * 1000\n",
    "    \n",
    "    # 3. Generate response\n",
    "    llm_start = time.time()\n",
    "    response_text, final_prompt = generate_response(query, retrieved_docs)\n",
    "    llm_latency = (time.time() - llm_start) * 1000\n",
    "    \n",
    "    total_latency = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # 4. Trace the pipeline execution\n",
    "    trace_result = tracer.trace_complete(\n",
    "        user_query=query,\n",
    "        final_prompt=final_prompt,\n",
    "        embedding=EmbeddingData(\n",
    "            vector=query_embedding,\n",
    "            retrieval_candidates=[\n",
    "                {\"doc_id\": doc[\"document_id\"], \"score\": doc[\"similarity_score\"]}\n",
    "                for doc in retrieved_docs\n",
    "            ]\n",
    "        ),\n",
    "        retrievals=[\n",
    "            RetrievalData(\n",
    "                document_id=doc[\"document_id\"],\n",
    "                similarity_score=doc[\"similarity_score\"],\n",
    "                metadata={\"text\": doc[\"content\"]}\n",
    "            )\n",
    "            for doc in retrieved_docs\n",
    "        ],\n",
    "        response=ResponseData(\n",
    "            text=response_text,\n",
    "            token_stream=response_text.split()\n",
    "        ),\n",
    "        telemetry=TelemetryData(\n",
    "            embedding_latency_ms=embedding_latency,\n",
    "            retrieval_latency_ms=retrieval_latency,\n",
    "            llm_latency_ms=llm_latency,\n",
    "            total_latency_ms=total_latency,\n",
    "            embedding_tokens=len(query.split()),\n",
    "            completion_tokens=len(response_text.split()),\n",
    "            api_cost=0.002  # Example cost\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response_text, trace_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example usage\n",
    "query = \"Who is the CEO of Tesla?\"\n",
    "response, trace = rag_pipeline(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Trace ID: {trace.get('id', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Tracing Example\n",
    "\n",
    "For high-throughput applications, you can use async tracing to avoid blocking the main pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up async tracer\n",
    "async_tracer = RAGTracer(api_url=\"http://localhost:8000\", async_mode=True)\n",
    "\n",
    "# Async version of the RAG pipeline\n",
    "def rag_pipeline_async(query):\n",
    "    # Record start time for telemetry\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Generate embedding for query\n",
    "    embedding_start = time.time()\n",
    "    query_embedding = generate_embedding(query)\n",
    "    embedding_latency = (time.time() - embedding_start) * 1000\n",
    "    \n",
    "    # 2. Retrieve documents\n",
    "    retrieval_start = time.time()\n",
    "    retrieved_docs = retrieve_documents(query_embedding)\n",
    "    retrieval_latency = (time.time() - retrieval_start) * 1000\n",
    "    \n",
    "    # 3. Generate response\n",
    "    llm_start = time.time()\n",
    "    response_text, final_prompt = generate_response(query, retrieved_docs)\n",
    "    llm_latency = (time.time() - llm_start) * 1000\n",
    "    \n",
    "    total_latency = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # 4. Trace the pipeline execution (async)\n",
    "    trace_result = async_tracer.trace_complete(\n",
    "        user_query=query,\n",
    "        final_prompt=final_prompt,\n",
    "        embedding=EmbeddingData(\n",
    "            vector=query_embedding,\n",
    "            retrieval_candidates=[\n",
    "                {\"doc_id\": doc[\"document_id\"], \"score\": doc[\"similarity_score\"]}\n",
    "                for doc in retrieved_docs\n",
    "            ]\n",
    "        ),\n",
    "        retrievals=[\n",
    "            RetrievalData(\n",
    "                document_id=doc[\"document_id\"],\n",
    "                similarity_score=doc[\"similarity_score\"],\n",
    "                metadata={\"text\": doc[\"content\"]}\n",
    "            )\n",
    "            for doc in retrieved_docs\n",
    "        ],\n",
    "        response=ResponseData(\n",
    "            text=response_text,\n",
    "            token_stream=response_text.split()\n",
    "        ),\n",
    "        telemetry=TelemetryData(\n",
    "            embedding_latency_ms=embedding_latency,\n",
    "            retrieval_latency_ms=retrieval_latency,\n",
    "            llm_latency_ms=llm_latency,\n",
    "            total_latency_ms=total_latency,\n",
    "            embedding_tokens=len(query.split()),\n",
    "            completion_tokens=len(response_text.split()),\n",
    "            api_cost=0.002  # Example cost\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response_text, trace_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example usage with async tracing\n",
    "query = \"Who is the CEO of SpaceX?\"\n",
    "response, trace = rag_pipeline_async(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Trace submission status: {trace.get('status', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebSocket Real-time Tracing\n",
    "\n",
    "For real-time monitoring, you can also use WebSockets to send trace data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import websocket\n",
    "import json\n",
    "\n",
    "# WebSocket client for real-time tracing\n",
    "def trace_via_websocket(trace_data):\n",
    "    ws = websocket.WebSocket()\n",
    "    ws.connect(\"ws://localhost:8000/ws/traces\")\n",
    "    ws.send(json.dumps(trace_data))\n",
    "    result = ws.recv()\n",
    "    ws.close()\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}